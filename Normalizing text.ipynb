{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correct spelling\n",
    "The [original source](https://github.com/wolfgarbe/SymSpell), but I am using a simple [python port](https://github.com/mammothb/symspellpy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from symspellpy.symspellpy import SymSpell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum edit distance per dictionary precalculation\n",
    "max_edit_distance_dictionary = 0\n",
    "prefix_length = 7\n",
    "# create object\n",
    "sym_spell = SymSpell(max_edit_distance_dictionary, prefix_length)\n",
    "# load dictionary\n",
    "dictionary_path = \"frequency_dictionary_en_82_765.txt\"\n",
    "term_index = 0  # column of the term in the dictionary text file\n",
    "count_index = 1  # column of the term frequency in the dictionary text file\n",
    "if not sym_spell.load_dictionary(dictionary_path, term_index, count_index):\n",
    "    print(\"Dictionary file not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd like to know how I'd done that !, 10, -58.50888790936667\n"
     ]
    }
   ],
   "source": [
    "input_term = \"I'd like toknowhow I'd done that!\"#\"thequickbrownfoxjumpsoverthelazydog\"\n",
    "\n",
    "result = sym_spell.word_segmentation(input_term)\n",
    "# display suggestion term, term frequency, and edit distance\n",
    "print(\"{}, {}, {}\".format(result.corrected_string, result.distance_sum,\n",
    "                          result.log_prob_sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing contractions:\n",
    "\n",
    "I will be using the [pycontractions](https://pypi.org/project/pycontractions/) library. It takes a three-pass approach. \n",
    "* First, the simple contractions with only a single rule are replaced. \n",
    "* On the second pass if any contractions are present with multiple rules we proceed to replace all combinations of rules to produce all possible texts. \n",
    "* Each text is then passed through a grammar checker and the Word Moverâ€™s Distance (WMD) is calculated between it and the original text. The hypotheses are then sorted by least number of grammatical errors and shortest distance from the original text and the top hypothesis is returned as the expanded form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycontractions import Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Softwares\\Anaconda3\\envs\\py36_nlp\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "# Load your favorite semantic vector model in gensim keyedvectors format from disk\n",
    "# cont = Contractions('GoogleNews-vectors-negative300.bin')\n",
    "# or specify any model from the gensim.downloader api\n",
    "cont = Contractions(api_key='glove-wiki-gigaword-50')\n",
    "# optional, prevents loading on first expand_texts call\n",
    "cont.load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I had like to know how I had done that!',\n",
       " 'we are going to the zoo and I do not think I will be home for dinner.',\n",
       " 'they are going to the zoo and she will be home for dinner.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cont.expand_texts([\"I'd like to know how I'd done that!\",\n",
    "                            \"We're going to the zoo and I don't think I'll be home for dinner.\",\n",
    "                            \"Theyre going to the zoo and she'll be home for dinner.\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I would like to know how I had done that!',\n",
       " 'we are going to the zoo and I do not think I will be home for dinner.',\n",
       " 'they are going to the zoo and she will be home for dinner.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cont.expand_texts([\"I'd like to know how I'd done that!\",\n",
    "                            \"We're going to the zoo and I don't think I'll be home for dinner.\",\n",
    "                            \"Theyre going to the zoo and she'll be home for dinner.\"], precise=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions to clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_pattern = re.compile('\\\"?([-a-zA-Z0-9.`?{}]+@\\w+\\.\\w+)\\\"?')\n",
    "dollar_and_decimals_pattern = re.compile('(\\$[-\\d]*\\.*\\d+)|(\\d*\\.\\d+)')\n",
    "us_phone_pattern = re.compile('\\d{3}-\\d{3}-\\d{4}|\\(\\d{3}\\)\\d{3}-\\d{4}')\n",
    "date_pattern = re.compile('\\s+\\d{1,4}[/-]\\d{1,2}[/-]*\\d{0,4}|\\s*\\d{1,4}[/-]\\d{1,2}[/-]*\\d{0,4}\\s+|\\d{2}-\\D{3}-\\d{2,4}')\n",
    "add_space_around_punct_patern = re.compile(r'([\\[.,!?():;\\]])')\n",
    "remove_multiple_space_pattern = re.compile('\\s{2,}|\\t')\n",
    "split_sentence_pattern = re.compile('[!&\\.;?]|\\*{2,}|\\-{2,}|/{2,}|,[\\s\\w]{25,},')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36_nlp",
   "language": "python",
   "name": "py36_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
