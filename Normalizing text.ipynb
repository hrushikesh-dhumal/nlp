{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting non ascii characters to ascii\n",
    "\n",
    "Sometimes because how the data was captured or stored weird charachters are added to the text. This and add noise to the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# represent Unicode in ASCII characters\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "À È Ì Ò Ù Ỳ Ǹ Ẁ\n",
      "ASCII representation:  A E I O U Y N W\n"
     ]
    }
   ],
   "source": [
    "text = 'À È Ì Ò Ù Ỳ Ǹ Ẁ'\n",
    "print(text)\n",
    "text = unidecode(text) # ascii rep of text\n",
    "print('ASCII representation: ', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correct spelling\n",
    "The [original source](https://github.com/wolfgarbe/SymSpell), but I am using a simple [python port](https://github.com/mammothb/symspellpy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from symspellpy.symspellpy import SymSpell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum edit distance per dictionary precalculation\n",
    "max_edit_distance_dictionary = 0\n",
    "prefix_length = 7\n",
    "# create object\n",
    "sym_spell = SymSpell(max_edit_distance_dictionary, prefix_length)\n",
    "# load dictionary\n",
    "dictionary_path = \"frequency_dictionary_en_82_765.txt\"\n",
    "term_index = 0  # column of the term in the dictionary text file\n",
    "count_index = 1  # column of the term frequency in the dictionary text file\n",
    "if not sym_spell.load_dictionary(dictionary_path, term_index, count_index):\n",
    "    print(\"Dictionary file not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd like to know how I'd done that !, 10, -58.50888790936667\n"
     ]
    }
   ],
   "source": [
    "input_term = \"I'd like toknowhow I'd done that!\"#\"thequickbrownfoxjumpsoverthelazydog\"\n",
    "\n",
    "result = sym_spell.word_segmentation(input_term)\n",
    "# display suggestion term, term frequency, and edit distance\n",
    "print(\"{}, {}, {}\".format(result.corrected_string, result.distance_sum,\n",
    "                          result.log_prob_sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing contractions:\n",
    "\n",
    "I will be using the [pycontractions](https://pypi.org/project/pycontractions/) library. It takes a three-pass approach. \n",
    "* First, the simple contractions with only a single rule are replaced. \n",
    "* On the second pass if any contractions are present with multiple rules we proceed to replace all combinations of rules to produce all possible texts. \n",
    "* Each text is then passed through a grammar checker and the Word Mover’s Distance (WMD) is calculated between it and the original text. The hypotheses are then sorted by least number of grammatical errors and shortest distance from the original text and the top hypothesis is returned as the expanded form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycontractions import Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Softwares\\Anaconda3\\envs\\py36_nlp\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "# Load your favorite semantic vector model in gensim keyedvectors format from disk\n",
    "# cont = Contractions('GoogleNews-vectors-negative300.bin')\n",
    "# or specify any model from the gensim.downloader api\n",
    "cont = Contractions(api_key='glove-wiki-gigaword-50')\n",
    "# optional, prevents loading on first expand_texts call\n",
    "cont.load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I had like to know how I had done that!',\n",
       " 'we are going to the zoo and I do not think I will be home for dinner.',\n",
       " 'they are going to the zoo and she will be home for dinner.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cont.expand_texts([\"I'd like to know how I'd done that!\",\n",
    "                            \"We're going to the zoo and I don't think I'll be home for dinner.\",\n",
    "                            \"Theyre going to the zoo and she'll be home for dinner.\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I would like to know how I had done that!',\n",
       " 'we are going to the zoo and I do not think I will be home for dinner.',\n",
       " 'they are going to the zoo and she will be home for dinner.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cont.expand_texts([\"I'd like to know how I'd done that!\",\n",
    "                            \"We're going to the zoo and I don't think I'll be home for dinner.\",\n",
    "                            \"Theyre going to the zoo and she'll be home for dinner.\"], precise=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions to clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_pattern = re.compile('\\\"?([-a-zA-Z0-9.`?{}]+@\\w+\\.\\w+)\\\"?')\n",
    "dollar_and_decimals_pattern = re.compile('(\\$[-\\d]*\\.*\\d+)|(\\d*\\.\\d+)')\n",
    "us_phone_pattern = re.compile('\\d{3}-\\d{3}-\\d{4}|\\(\\d{3}\\)\\d{3}-\\d{4}')\n",
    "date_pattern = re.compile('\\s+\\d{1,4}[/-]\\d{1,2}[/-]*\\d{0,4}|\\s*\\d{1,4}[/-]\\d{1,2}[/-]*\\d{0,4}\\s+|\\d{2}-\\D{3}-\\d{2,4}')\n",
    "add_space_around_punct_patern = re.compile(r'([\\[.,!?():;\\]])')\n",
    "remove_multiple_space_pattern = re.compile('\\s{2,}|\\t')\n",
    "split_sentence_pattern = re.compile('[!&\\.;?]|\\*{2,}|\\-{2,}|/{2,}|,[\\s\\w]{25,},')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "from fasttext import tokenize\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# I am using lid.176.bin, which is faster and slightly more accurate,\n",
    "# there is a compressed version avaiable at https://fasttext.cc/docs/en/language-identification.html\n",
    "fasttext_language_model = fasttext.load_model(os.path.join(\"model\", \"lid.176.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_space_pattern = re.compile(r\"\\s\")\n",
    "def preprocess_text_for_language_detection(text: str):\n",
    "    \"\"\"\n",
    "    Cleans the text as per fasttext requirements.\n",
    "    The requirements can be found here: https://pypi.org/project/fasttext/\n",
    "    :text: str: text to clean\n",
    "    :returns: str: cleaned text\n",
    "    \"\"\"\n",
    "    # fastText assumes UTF-8 encoded text\n",
    "    text = str(text)\n",
    "    \n",
    "    # fastText is not aware of UTF-8 whitespace\n",
    "    # Replace all white space with space\n",
    "    text = white_space_pattern.sub(text, \" \")\n",
    "    \n",
    "    # Tokenize text, per fastext function and rejoin\n",
    "    tokens = tokenize(text)\n",
    "    text = \" \".join(tokens)\n",
    "    n = len(tokens)\n",
    "    \n",
    "    # Remove white space char as it affects the model accuracy\n",
    "    text = text.replace(\"</s>\", \"\")\n",
    "    \n",
    "    return text.lower()\n",
    "\n",
    "def identify_languages(text: str, no_of_languages: int =1):\n",
    "    \"\"\"\n",
    "    Uses fasttext language detection and some simple cleaning \n",
    "    to identify languages in text.\n",
    "    Returns language code from here -  https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes\n",
    "    :text: str:\n",
    "    :no_of_languages: int\n",
    "    :returns: list of tuples containing language and its probability\n",
    "    \"\"\"\n",
    "    clean_text = preprocess_text_for_language_detection(text)\n",
    "    ft_output = fasttext_language_model.predict(text, no_of_languages)\n",
    "    # format output\n",
    "    result = [(ft_output[0][i][-2:], ft_output[1][i]) for i in range(len(ft_output[0]))]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('es', 1.0000468492507935)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identify_languages(\"¿Cómo estás\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacremoses import MosesTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = MosesTokenizer(lang='en')\n",
    "text = u'This, is a sentence with weird\\xbb symbols\\u2026 appearing everywhere\\xbf'\n",
    "tokenized_text = mt.tokenize(text, return_str=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This , is a sentence with weird » symbols … appearing everywhere ¿'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacremoses import M"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_nlp",
   "language": "python",
   "name": "py37_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
